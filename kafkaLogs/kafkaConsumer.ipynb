{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/divaamahajan/Spark-Logs/blob/kafkaLogs/kafkaConsumer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFpfAko2R13W"
   },
   "source": [
    "## Setting up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8C36yU3dsTWK",
    "outputId": "e4a7e7ae-6449-43d4-b626-064f35718ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting confluent-kafka==2.1.1\n",
      "  Downloading confluent_kafka-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kafka==1.3.5\n",
      "  Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kafka-python==2.0.2\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka==2.1.1\n",
    "!pip install kafka==1.3.5\n",
    "!pip install kafka-python==2.0.2\n",
    "!pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDwUc5sBsoEZ",
    "outputId": "2de919ca-e98b-41d6-9a96-71f54567dc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark==3.4.0\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=0e31bf55dd18e5d197038ab1f3798d6b324441b87eef08fc72bad76b3d59e85d\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pytz==2023.3\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.7.1\n",
      "    Uninstalling pytz-2022.7.1:\n",
      "      Successfully uninstalled pytz-2022.7.1\n",
      "Successfully installed pytz-2023.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting spark==0.2.1\n",
      "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: spark\n",
      "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58747 sha256=fcc8d83ee254139b17dc532a089a089a45c40bdb978b2be4e25494a402e6d7fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/88/77/b4131110ea4094540f7b47c6d62a649807d7e94800da5eab0b\n",
      "Successfully built spark\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (2023.3)\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "(Reading database ... 122545 files and directories currently installed.)\n",
      "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install pkg_resources==0.0.0\n",
    "!pip install py4j==0.10.9.7\n",
    "!pip install pyspark==3.4.0\n",
    "!pip install python-dateutil==2.8.2\n",
    "!pip install pytz==2023.3\n",
    "!pip install six==1.16.0\n",
    "!pip install spark==0.2.1\n",
    "!pip install tzdata==2023.3\n",
    "!apt-get install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNWmFL2ItKPv",
    "outputId": "eaff527b-e038-4ddd-ea4e-ddef282c2a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: confluent_kafka in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Collecting sseclient\n",
      "  Downloading sseclient-0.0.27.tar.gz (7.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting kafka\n",
      "  Using cached kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: requests>=2.9 in /usr/local/lib/python3.10/dist-packages (from sseclient) (2.27.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sseclient) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (3.4)\n",
      "Building wheels for collected packages: sseclient\n",
      "  Building wheel for sseclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sseclient: filename=sseclient-0.0.27-py3-none-any.whl size=5565 sha256=61d6dd5079fa7cf3cf90859aaa5a0175652d97d49dd4f0fa5e4b0cf66c0ff856\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/57/0e/09b1264923280e935a34cc543b7f147f5df12490bd7a992f42\n",
      "Successfully built sseclient\n",
      "Installing collected packages: kafka, sseclient\n",
      "Successfully installed kafka-1.3.5 sseclient-0.0.27\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka sseclient kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuPIie_dtWKX",
    "outputId": "fc2bcb29-57f0-41e1-bf04-f6f46c324165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kafka 1.3.5\n",
      "Uninstalling kafka-1.3.5:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/kafka-1.3.5.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/kafka/*\n",
      "Proceed (Y/n)? Y\n",
      "  Successfully uninstalled kafka-1.3.5\n",
      "\u001b[33mWARNING: Skipping kafka-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall kafka\n",
    "!pip uninstall kafka-python\n",
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "viipdG4ZTLYd"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import from_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXaXPwRgYQRW"
   },
   "source": [
    "## Set up the Kafka consumer configuration and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mzDHy_vuYSxp"
   },
   "outputs": [],
   "source": [
    "# Specify the Kafka broker and topic to consume from\n",
    "brokers = 'localhost:9092'\n",
    "topic = 'log_topic'\n",
    "conf = {\n",
    "    'bootstrap.servers': brokers,\n",
    "    'group.id': 'python-consumer',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'api.version.request': True\n",
    "}\n",
    "parquet_host = 'localhost:9000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-WFDfx9FUoIN"
   },
   "outputs": [],
   "source": [
    "dataset = '42MBSmallServerLog.log'\n",
    "zip_file_path = f'{dataset}.zip'\n",
    "extracted_folder_path = 'extracted_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmZihqH8UXp8"
   },
   "source": [
    "## 1. Set up the Kafka consumer \n",
    "Kafka Consumer with Spark (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P4hgtiajYqmq"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StringType\n",
    "# Import the KafkaConsumer from confluent_kafka\n",
    "from confluent_kafka import KafkaException, KafkaError\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Function to process the log lines RDD\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_logs(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        # Convert RDD to DataFrame\n",
    "        logs_df = spark.read.json(rdd)\n",
    "\n",
    "        # Data wrangling steps\n",
    "        host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "        ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "        method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "        status_pattern = r'\\s(\\d{3})\\s'\n",
    "        content_size_pattern = r'\\s(\\d+)$'\n",
    "\n",
    "        logs_df = logs_df.withColumn('host', regexp_extract('value', host_pattern, 1))\n",
    "        logs_df = logs_df.withColumn('timestamp', regexp_extract('value', ts_pattern, 1))\n",
    "        logs_df = logs_df.withColumn('method', regexp_extract('value', method_uri_protocol_pattern, 1))\n",
    "        logs_df = logs_df.withColumn('endpoint', regexp_extract('value', method_uri_protocol_pattern, 2))\n",
    "        logs_df = logs_df.withColumn('protocol', regexp_extract('value', method_uri_protocol_pattern, 3))\n",
    "        logs_df = logs_df.withColumn('status', regexp_extract('value', status_pattern, 1).cast('integer'))\n",
    "        logs_df = logs_df.withColumn('content_size', regexp_extract('value', content_size_pattern, 1).cast('integer'))\n",
    "\n",
    "        # Add columns for year, month, and date\n",
    "        logs_df = logs_df.withColumn('year', year(logs_df['timestamp']))\n",
    "        logs_df = logs_df.withColumn('month', month(logs_df['timestamp']))\n",
    "        logs_df = logs_df.withColumn('date', dayofmonth(logs_df['timestamp']))\n",
    "\n",
    "        # Add columns for hour, minute, and second\n",
    "        logs_df = logs_df.withColumn('hour', hour(logs_df['timestamp']))\n",
    "        logs_df = logs_df.withColumn('minute', minute(logs_df['timestamp']))\n",
    "        logs_df = logs_df.withColumn('second', second(logs_df['timestamp']))\n",
    "\n",
    "        # Print the transformed DataFrame\n",
    "        logs_df.show(10, truncate=True)\n",
    "\n",
    "        # Store the transformed DataFrame as Parquet files\n",
    "        logs_df.write.parquet('logs.parquet')\n",
    "\n",
    "        # Store the Parquet files in HDFS\n",
    "        # writes the transformed DataFrame to the local filesystem as Parquet files with the name logs.parquet. \n",
    "        # These Parquet files are created in the same directory where the consumer.py script is executed.\n",
    "        logs_df.write.parquet('hdfs://localhost:9000/logs.parquet')\n",
    "\n",
    "        # Download the processed DataFrame as CSV\n",
    "        logs_df.write.csv('processed_logs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_kafka_stream(consumer):\n",
    "    num = 0\n",
    "    while True:\n",
    "        try:\n",
    "            message = consumer.poll(1.0)  # Poll for a single message\n",
    "            if not message: \n",
    "                print(\"no msg\")\n",
    "                continue\n",
    "            if message.error():\n",
    "                print(\"Consumer error: {}\".format(message.error()))\n",
    "                continue\n",
    "            num += 1\n",
    "            print(\"before\",num)\n",
    "            yield message.value().decode('utf-8')\n",
    "            print(\"after\",num)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRVXbX48Zht8",
    "outputId": "cf59de62-0a0f-4980-af8f-115b8881f05c"
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('KafkaConsumer').getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 10 seconds\n",
    "ssc = StreamingContext(spark.sparkContext, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "p0l-zgbBUZZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Consumer has been initiated...\n"
     ]
    }
   ],
   "source": [
    "# Set up the Kafka consumer\n",
    "consumer = Consumer(conf)\n",
    "print('Kafka Consumer has been initiated...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HABkv8fpaXrx"
   },
   "source": [
    "### Subscribe to the Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vVbzDYMJaVLx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available topics to consume:  {'log_lines_topic': TopicMetadata(log_lines_topic, 1 partitions), 'log_topic': TopicMetadata(log_topic, 1 partitions), '__consumer_offsets': TopicMetadata(__consumer_offsets, 50 partitions)}\n",
      "Subscribed to topic: log_topic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Determine the name of the topic that should be consumed and subscribe to it.\n",
    "topics = consumer.list_topics().topics\n",
    "print('Available topics to consume: ', topics)\n",
    "\n",
    "# Subscribe to the Kafka topic\n",
    "if topic in topics:\n",
    "    consumer.subscribe([topic])\n",
    "    print('Subscribed to topic:', topic)\n",
    "else:\n",
    "    print('Topic', topic, 'does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a streaming DataFrame from Kafka\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrokers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/c/Users/Divya Mahajan/OneDrive/Documents/gitcode/Spark-Logs/myenv/lib/python3.8/site-packages/pyspark/sql/streaming/readwriter.py:277\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/c/Users/Divya Mahajan/OneDrive/Documents/gitcode/Spark-Logs/myenv/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/Users/Divya Mahajan/OneDrive/Documents/gitcode/Spark-Logs/myenv/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# Create a streaming DataFrame from Kafka\n",
    "df = (spark\n",
    "      .readStream\n",
    "      .format('kafka')\n",
    "      .option('kafka.bootstrap.servers', brokers)\n",
    "      .option('subscribe', topic)\n",
    "      .option('startingOffsets', 'earliest')\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPGFkKXnafte"
   },
   "outputs": [],
   "source": [
    "kafka_stream = ssc.queueStream([get_kafka_stream()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8E0v57o-c-fW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the log lines from the Kafka stream\n",
    "lines = kafka_stream.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqaY05WZdGS4"
   },
   "source": [
    "### convert the log lines to DataFrame and apply SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idTSDwjAdLDv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to process the log lines RDD\n",
    "def process_logs(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        # Convert RDD to DataFrame\n",
    "        logs_df = spark.read.json(rdd)\n",
    "\n",
    "        # Perform required transformations and exploratory data analysis\n",
    "        # For example, you can apply SQL queries, aggregations, or filters on the DataFrame\n",
    "        transformed_df = logs_df.select('host', 'timestamp', 'method', 'endpoint', 'protocol', 'status', 'content_size')\n",
    "\n",
    "        # Print the transformed DataFrame\n",
    "        transformed_df.show()\n",
    "\n",
    "        # Store the transformed DataFrame as Parquet files\n",
    "        transformed_df.write.parquet('logs.parquet')\n",
    "\n",
    "        # Store the Parquet files in HDFS\n",
    "        transformed_df.write.parquet(f'hdfs://{parquet_host}/logs.parquet')\n",
    "\n",
    "lines.foreachRDD(lambda rdd: process_logs(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImXehOhwdRGG"
   },
   "source": [
    "### Start the streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmZLh7IZdUgt"
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIEU6iUDdmMc"
   },
   "source": [
    "## 4. Parquet File Creation (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlwaaQNwdpj2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('ParquetFileCreation').getOrCreate()\n",
    "\n",
    "# Read the log data file into a DataFrame\n",
    "logs_rdd = spark.sparkContext.textFile(f'{dataset}')\n",
    "logs_df = spark.createDataFrame(logs_rdd, \"string\").toDF(\"log_line\")\n",
    "\n",
    "# Perform required transformations and analysis on the log data\n",
    "# For example, you can extract structured attributes using regular expressions\n",
    "transformed_df = process_logs(logs_df)\n",
    "\n",
    "# Store the transformed DataFrame as Parquet files\n",
    "transformed_df.write.parquet('logs.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRD5F5UzdvHK"
   },
   "source": [
    "## 5. Storing Parquet Files in HDFS (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R62C7hIydx3W"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('ParquetFileHDFS').getOrCreate()\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "logs_df = spark.read.parquet('logs.parquet')\n",
    "\n",
    "# Store the Parquet files in HDFS\n",
    "logs_df.write.parquet(f'hdfs://{parquet_host}/logs.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMxDQY+YPzj7xiRefFxzyfM",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
