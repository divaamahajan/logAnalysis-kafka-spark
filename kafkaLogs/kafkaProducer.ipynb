{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/divaamahajan/Spark-Logs/blob/kafkaLogs/kafkaProducer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFpfAko2R13W"
   },
   "source": [
    "## Setting up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8C36yU3dsTWK",
    "outputId": "e4a7e7ae-6449-43d4-b626-064f35718ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting confluent-kafka==2.1.1\n",
      "  Downloading confluent_kafka-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kafka==1.3.5\n",
      "  Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kafka-python==2.0.2\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka==2.1.1\n",
    "!pip install kafka==1.3.5\n",
    "!pip install kafka-python==2.0.2\n",
    "!pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDwUc5sBsoEZ",
    "outputId": "2de919ca-e98b-41d6-9a96-71f54567dc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark==3.4.0\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=0e31bf55dd18e5d197038ab1f3798d6b324441b87eef08fc72bad76b3d59e85d\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pytz==2023.3\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.7.1\n",
      "    Uninstalling pytz-2022.7.1:\n",
      "      Successfully uninstalled pytz-2022.7.1\n",
      "Successfully installed pytz-2023.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting spark==0.2.1\n",
      "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: spark\n",
      "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58747 sha256=fcc8d83ee254139b17dc532a089a089a45c40bdb978b2be4e25494a402e6d7fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/88/77/b4131110ea4094540f7b47c6d62a649807d7e94800da5eab0b\n",
      "Successfully built spark\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (2023.3)\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "(Reading database ... 122545 files and directories currently installed.)\n",
      "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install pkg_resources==0.0.0\n",
    "!pip install py4j==0.10.9.7\n",
    "!pip install pyspark==3.4.0\n",
    "!pip install python-dateutil==2.8.2\n",
    "!pip install pytz==2023.3\n",
    "!pip install six==1.16.0\n",
    "!pip install spark==0.2.1\n",
    "!pip install tzdata==2023.3\n",
    "!apt-get install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNWmFL2ItKPv",
    "outputId": "eaff527b-e038-4ddd-ea4e-ddef282c2a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: confluent_kafka in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Collecting sseclient\n",
      "  Downloading sseclient-0.0.27.tar.gz (7.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting kafka\n",
      "  Using cached kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: requests>=2.9 in /usr/local/lib/python3.10/dist-packages (from sseclient) (2.27.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sseclient) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9->sseclient) (3.4)\n",
      "Building wheels for collected packages: sseclient\n",
      "  Building wheel for sseclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sseclient: filename=sseclient-0.0.27-py3-none-any.whl size=5565 sha256=61d6dd5079fa7cf3cf90859aaa5a0175652d97d49dd4f0fa5e4b0cf66c0ff856\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/57/0e/09b1264923280e935a34cc543b7f147f5df12490bd7a992f42\n",
      "Successfully built sseclient\n",
      "Installing collected packages: kafka, sseclient\n",
      "Successfully installed kafka-1.3.5 sseclient-0.0.27\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka sseclient kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuPIie_dtWKX",
    "outputId": "fc2bcb29-57f0-41e1-bf04-f6f46c324165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kafka 1.3.5\n",
      "Uninstalling kafka-1.3.5:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/kafka-1.3.5.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/kafka/*\n",
      "Proceed (Y/n)? Y\n",
      "  Successfully uninstalled kafka-1.3.5\n",
      "\u001b[33mWARNING: Skipping kafka-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall kafka\n",
    "!pip uninstall kafka-python\n",
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "viipdG4ZTLYd"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXaXPwRgYQRW"
   },
   "source": [
    "## Set up Topic and broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mzDHy_vuYSxp"
   },
   "outputs": [],
   "source": [
    "# Specify the Kafka broker and topic to consume from\n",
    "brokers = 'localhost:9092'\n",
    "topic = 'log_lines_topic'\n",
    "parquet_host = 'localhost:9000'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knvxo8xXUjhN"
   },
   "source": [
    "## 1. Extract the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-WFDfx9FUoIN"
   },
   "outputs": [],
   "source": [
    "dataset = '42MBSmallServerLog.log'\n",
    "zip_file_path = f'{dataset}.zip'\n",
    "extracted_folder_path = 'extracted_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sYfyMTdIUg5a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYp3yQ4WUPib"
   },
   "source": [
    "## 2. Set up the Kafka producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_z4j0eFyuDd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Producer has been initiated...\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "producer = Producer({\n",
    "    'bootstrap.servers': brokers,  # Kafka broker address\n",
    "    'api.version.request': True\n",
    "})\n",
    "\n",
    "print('Kafka Producer has been initiated...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-T2IKSYVCqS"
   },
   "source": [
    "## Read log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receipt(err,msg):\n",
    "    if err is not None:\n",
    "        print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I3EMsQvwVHsD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data flushed to Producer Queues\n"
     ]
    }
   ],
   "source": [
    "# Read log file\n",
    "log_file_path = f'{extracted_folder_path}/{dataset}'\n",
    "batch_size = 1000  # Adjust this value based on requirements\n",
    "# Open the log file\n",
    "with open(log_file_path, 'r') as file:\n",
    "    batch = []\n",
    "    for line in file:\n",
    "        # Add each line to the current batch\n",
    "        batch.append(line)\n",
    "        # Check if the batch size is reached\n",
    "        if len(batch) >= batch_size:\n",
    "            # Produce the batch to Kafka\n",
    "            # producer.send(topic, value=''.join(batch).encode('utf-8'))\n",
    "            producer.produce(topic, value=''.join(batch).encode('utf-8'))\n",
    "            # producer.flush()\n",
    "            # Clear the batch\n",
    "            batch = []\n",
    "    # Check if there are any remaining lines in the last batch\n",
    "    if batch:\n",
    "        # Produce the remaining lines to Kafka\n",
    "        producer.poll(1)\n",
    "        producer.produce(topic, value=''.join(batch).encode('utf-8'),callback=receipt)\n",
    "        # producer.flush()\n",
    "# Flush any remaining messages in the producer queue\n",
    "producer.flush()\n",
    "print(\"Data flushed to Producer Queues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPj1sA1YKj38k5YIATI3zHP",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
